\chapter{Systematic Uncertainties in the Short Baseline Neutrino Program}

In the previous chapter, the motivation for the Fermilab Short Baseline Neutrino program was presented and the expected event rates were shown, as well as the methods of calculating an expected signal from a 3+1 model.  However, the most detailed simulation (or data analysis, for that matter) is not consequential without a robust calculation of systematic uncertainties.

In this chapter, the systematic uncertainties for the Short Baseline are discussed.  Of particular importance are the uncertainties from the flux and neutrino interactions.  The flux for the Booster Neutrino Beam, while among the best known neutrino beam fluxes, still has residual uncertainties of up to 15\% \cite{miniboone_flux_paper}.  Similarly, the uncertainty in the model of neutrino interactions has a 10 to 15\% normalization uncertainty for the quasi-elastic and resonant events that are most important to the oscillation searches.  Considering that the amplitude of any sterile neutrino oscillation effect is very small, with oscillation probabilities that peak at 1\% or less, constraining the systematic uncertainties in the Short Baseline Program is absolutely essential.

The strength of the Short Baseline Program's oscillation search comes, ultimately, from two factors:  the \lartpc technology allows excellent event identification and background rejections, and the near detector, SBND, allows for large cancellation of systematic uncertainties.  In this chapter, the method for quantifying the cancellation of systematic uncertainties is presented.


\section{General Framework for quantification of uncertainties}

In this analysis, the uncertainties that matter are the systematic uncertainties on the final distribution of event rates.  Since the goal is to produce a sensitivity calculation for an expected signal, the numerical value of the sensitivity can be calculated with a $\chi^2$ calculation:

\begin{equation}
\begin{centering}
\chi^2(\Delta m^2, \text{sin}^2 2 \theta ) = \sum_{i,j} [N^{null}_i - N^{osc}_i(\Delta m^2, \text{sin}^2 2 \theta ) ] \times E^{-1}_{i,j} \times [N^{null}_j - N^{osc}_j(\Delta m^2, \text{sin}^2 2 \theta ) ],
\end{centering}
\end{equation}

where $N^{null}_i$ is the expected event rate in the $i^{th}$ analysis bin with no oscillation signal, and $N^{osc}_i(\Delta m^2, \text{sin}^2 2 \theta )$ is the expected event rate in the $i^{th}$ analysis bin if there is an oscillation signal from a 3+1 model with the specified mass splitting and amplitude.  In the \nue appearance analysis, this is simplified to 
\begin{equation}
\begin{centering}
N^{null}_i - N^{osc}_i(\Delta m^2, \text{sin}^2 2 \theta ) = S_i(\Delta m^2, \text{sin}^2 2 \theta )
\end{centering}
\end{equation}
where S is the expected signal events from the specified parameters in the $i^{th}$ bin.

$E_{i,j}$ in the $\chi^2$ computation is the covariance matrix, a statistical tool to encode correlated uncertainties.  In practice, the computation of the covariance matrix is the most challenging aspect of the $\chi^2$ calculation because it requires careful determination of how the uncertainties under study are correlated.  For this work, the correlations of uncertainties are quantified with the ``multiple universe'' method \footnote{Nothing to do with the cosmological idea of the multiverse}.  Much more will be said about the computation and use of the covariance matrix in Section~\ref{sec:covariance_matrix}.

\subsection{Multiple Universe Error Propagation and Reweighing methods}

In a complex chain of simulation and analysis such as a prediction of event rates in a neutrino detector, it can be challenging to understand the effect of, for example, an uncertainty of hadron production at the proton target on the final distribution of neutrino events in the detector.  Some intuitive knowledge is of course present: if the amount of neutrino producing particles generated at the target by proton interactions is under (or over) estimated, the event rates in the final analysis distribution at the detector will also be under (over) estimated.  To precisely quantify the relationship between initial variable underlying the simulation and the final distributions of events, a reweighing scheme with multiple universes is used.

\subsubsection{Reweighing Events}

As a an example, 



Multiverse analysis, drawing from physical distributions and propagating to the end distributions.  Reweighting method.

\section{Determination of Covariance Matrices}
\label{sec:covariance_matrix}

Using the methods described above for applying weights on an event-by-event basis, it's possible to generate a suite of ``Universes''
of event rate histograms, where the value of each analysis bin can be known in each universe as $N^i_{\text{Univ.} m}.$  In this document, since there are three detectors under consideration, the vector of event rates in each analysis bin, $N$, is a concatenation of the vector of event rates in each detector.  If there are $P$ total analysis bins in each detector, then 
\begin{equation}
\begin{centering}
\vec{N}_{\text{Nom.}} = \left(~N_{\text{Nom.}}^{1,~SBND},~\dots~N_{\text{Nom.}}^{P,~SBND},~N_{\text{Nom.}}^{1,~\uboone}~\dots~N_{\text{Nom.}}^{P,~\uboone},~N_{\text{Nom.}}^{1,~\icarus}~\dots~N_{\text{Nom.}}^{P,~\icarus} ~\right)
\end{centering}
\end{equation}
and in each universe where an underlying physical parameter has been varied:
\begin{equation}
\begin{centering}
\vec{N}_{\text{Univ.}~m} = \left(~N_{\text{Univ.}~m}^{1,~SBND},~\dots~N_{\text{Univ.}~m}^{P,~SBND},~N_{\text{Univ.}~m}^{1,~\uboone}~\dots~N_{\text{Univ.}~m}^{P,~\uboone},~N_{\text{Univ.}~m}^{1,~\icarus}~\dots~N_{\text{Univ.}~m}^{P,~\icarus} ~\right).
\end{centering}
\end{equation}

With these vectors, it's possible to calculate deviation from the nominal values due to the underlying uncertainties in an analysis bin:
\begin{equation}
\begin{centering}
\sigma^i = \sqrt{\frac{1}{M}\sum_{\text{All Univ.}~m}^{M} \left( N^i_{\text{Nom.}} - N^i_{\text{Univ.~m}}\right)^2}
\end{centering}
\label{eq:bin_uncert}
\end{equation}

This measurement of the uncertainty in this way gives an estimate of the uncertainty in single detector experiments, where bin to bin correlations are ignored.  In other words, $\sigma^i$ is the uncertainty in the $i^{th}$ analysis bin when the existence of all the other bins, in any detector, are ignored.  See Figures~\ref{fig:sys_flux_uncert_fracUncert}, \ref{fig:sys_xsec_uncert_fracUncert} for this measurement due to flux and cross section uncertainties, below.  In a practical sense, this measurement of the uncertainty is not useful for the computation of sensitivities or significances of a signal, but only provides an easily interpreted measure of the uncertainty of a single detector experiment.

A more useful statical tool is the covariance matrix, $E$, defined at each bin as
\begin{equation}
\begin{centering}
E^{i,j} = \frac{1}{M}\sum_{\text{All Univ.}~m}^{M} \left[ N^i_{\text{Nom.}} - N^i_{\text{Univ.~m}}\right] \times \left[ N^j_{\text{Nom.}} - N^j_{\text{Univ.~m}}\right].
\end{centering}
\label{eq:cov_mat}
\end{equation}

Covariance matrices that arise from uncertainty sources that are uncorrelated are separable, in the sense that for a complete analysis the final covariance matrix can be constructed as the sum of the matrices from each source.  In this analysis, a covariance matrix is calculated for the flux and cross section uncertainties for beam intrinsic events, and the matrix is estimated for the backgrounds from ``Dirt'' and cosmic induced events, as well as detector systematics.
\begin{equation}
\begin{centering}
E = E_{\text{Stat.}} + E_{\text{Flux}} + E_{\text{Cross Section}} + E_{\text{Dirt}} + E_{\text{Cosmic}} + E_{\text{Det. Syst.}}
\end{centering}
\end{equation}

The covariance matrix is more easily visualized in the form of some of it's transforms, the fractional covariance matrix
\begin{equation}
\begin{centering}
F^{i,j} \equiv \frac{E^{i,j}}{N^{i} N^{j}}
\end{centering}
\end{equation}
and the correlation matrix
\begin{equation}
\begin{centering}
C^{i,j} \equiv \frac{ E^{i,j} }{ \sqrt{E^{i,i}} \sqrt{E^{j,j}} }.
\end{centering}
\end{equation}

See Figures~\ref{fig:syst_flux_fracmatrix}, \ref{fig:syst_xsec_fracmatrix} for examples of the fractional covariance matrix, and Figures~\ref{fig:syst_flux_corrmatrix}, \ref{fig:syst_xsec_corrmatrix} for examples of the correlation matrix.  The fractional error matrix shows which analysis bins have the largest systematic uncertainty, though because it is relative it can be deceiving: bins with high systematic uncertainties might not be important bins in the analysis. 

The correlation matrix is an excellent visualization of the power of the covariance matrix technique.  It is limited to between -1 (full anticorrelation) and 1 (full correlation), and each entry at bin $(i,j)$ displays how correlated the $i^{th}$ bin is to the $j^{th}$ bin.  This is the vital information that allows correlated uncertainties in a multi detector experiment to cancel: a deviation at the far detector becomes significant (even if it is within the nominal uncertainty at that bin given by Eq. \ref{eq:bin_uncert}) if the deviation is not seen at a near detector {\bf and} the correlation between near and far is large.  The correlation matrices show the magnitude of exactly that correlation, while the covariance matrix (\ref{eq:cov_mat}) is the mathematical tool that carries correlation information to the $\chi^2$ calculation.


\section{Uncertainties from Neutrino Flux}

\label{section:flux_uncert}

As might be expected, the neutrino flux is highly correlated across the three detectors in the Booster Neutrino Beam.

\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{systematics_figures/matrixFile_nue_ND_100m_uB_T600_onaxis_flux_6_ecalo2_nu_vePhot0.05_gap3_fracUncert}
    \caption{The predicted event rates for the SBN program in all three detectors, assuming 2.2e20 Protons on Target delivered each year.  For this analysis, \uboone is assumed to have 6 years of running (its original 3 + 3 with the SBN program)}.
   \label{fig:sys_flux_uncert_fracUncert}
\end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{systematics_figures/matrixFile_nue_ND_100m_uB_T600_onaxis_flux_6_ecalo2_nu_vePhot0.05_gap3_fracMatHist}
    \caption{The predicted event rates for the SBN program in all three detectors, assuming 2.2e20 Protons on Target delivered each year.  For this analysis, \uboone is assumed to have 6 years of running (its original 3 + 3 with the SBN program)}.
   \label{fig:syst_flux_fracmatrix}
\end{figure}
\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{systematics_figures/matrixFile_nue_ND_100m_uB_T600_onaxis_flux_6_ecalo2_nu_vePhot0.05_gap3_corrMatHist}
    \caption{The predicted event rates for the SBN program in all three detectors, assuming 2.2e20 Protons on Target delivered each year.  For this analysis, \uboone is assumed to have 6 years of running (its original 3 + 3 with the SBN program)}.
   \label{fig:syst_flux_corrmatrix}
\end{figure}



Detailed discussion of the origins of the flux uncertainties.  The physical parameters that we have to constrain, and where the uncertainty ranges come from.  Dead-reckoned uncertainties in the event rates for each uncertainty, if possible, and dead-reckoned total uncertainty at each detector.
\cite{AguilarArevalo:2008yp}

\section{Uncertainties from Neutrino Interactions}

\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{systematics_figures/matrixFile_nue_ND_100m_uB_T600_onaxis_xsec_0_ecalo2_nu_vePhot0.05_gap3_fracUncert}
    \caption{The predicted event rates for the SBN program in all three detectors, assuming 2.2e20 Protons on Target delivered each year.  For this analysis, \uboone is assumed to have 6 years of running (its original 3 + 3 with the SBN program)}.
   \label{fig:sys_xsec_uncert_fracUncert}
\end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{systematics_figures/matrixFile_nue_ND_100m_uB_T600_onaxis_xsec_0_ecalo2_nu_vePhot0.05_gap3_fracMatHist}
    \caption{The predicted event rates for the SBN program in all three detectors, assuming 2.2e20 Protons on Target delivered each year.  For this analysis, \uboone is assumed to have 6 years of running (its original 3 + 3 with the SBN program)}.
   \label{fig:syst_xsec_fracmatrix}
\end{figure}
\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{systematics_figures/matrixFile_nue_ND_100m_uB_T600_onaxis_xsec_0_ecalo2_nu_vePhot0.05_gap3_corrMatHist}
    \caption{The predicted event rates for the SBN program in all three detectors, assuming 2.2e20 Protons on Target delivered each year.  For this analysis, \uboone is assumed to have 6 years of running (its original 3 + 3 with the SBN program)}.
   \label{fig:syst_xsec_corrmatrix}
\end{figure}

\section{Residual Systematic Uncertainties}